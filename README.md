# Data Embeddings with Word2Vec

This project demonstrates how to build and visualize word embeddings using the Word2Vec model from the `gensim` library. It covers the process of preprocessing textual data, training a Word2Vec model, and visualizing the resulting word vectors using dimensionality reduction techniques like PCA and t-SNE.

---

## ğŸ“Œ Features

- Text preprocessing with NLTK
- Word2Vec embedding training using `gensim`
- Dimensionality reduction with PCA and t-SNE
- Visualization of word vectors in 2D space
- Interactive insights into word similarity and clustering

---

## ğŸ§  Technologies Used

- Python
- Gensim
- NLTK
- Scikit-learn
- Matplotlib
- Seaborn

---

## ğŸ› ï¸ Installation

1. **Clone the Repository**

```bash
git clone https://github.com/saranjthilak/data-embeddings-with-word2vec.git
cd data-embeddings-with-word2vec
```
## ğŸ“Š Output
Trained Word2Vec model (.model file)

2D scatter plot of word embeddings

Visual clusters of semantically similar words

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ data/                      # Input text files
â”œâ”€â”€ models/                    # Saved Word2Vec models
â”œâ”€â”€ outputs/                   # Embedding plots and results
â”œâ”€â”€ word2vec_training.py       # Training script
â”œâ”€â”€ visualize_embeddings.py    # Visualization script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
## ğŸ“ License
This project is licensed under the MIT License. See the LICENSE file for details.

## ğŸ¤ Contributing
Contributions, issues, and feature requests are welcome!
Feel free to open a pull request or an issue.

