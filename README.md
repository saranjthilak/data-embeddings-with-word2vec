# Data Embeddings with Word2Vec

This project demonstrates how to build and visualize word embeddings using the Word2Vec model from the `gensim` library. It covers the process of preprocessing textual data, training a Word2Vec model, and visualizing the resulting word vectors using dimensionality reduction techniques like PCA and t-SNE.

---

## 📌 Features

- Text preprocessing with NLTK
- Word2Vec embedding training using `gensim`
- Dimensionality reduction with PCA and t-SNE
- Visualization of word vectors in 2D space
- Interactive insights into word similarity and clustering

---

## 🧠 Technologies Used

- Python
- Gensim
- NLTK
- Scikit-learn
- Matplotlib
- Seaborn

---

## 🛠️ Installation

1. **Clone the Repository**

```bash
git clone https://github.com/saranjthilak/data-embeddings-with-word2vec.git
cd data-embeddings-with-word2vec
```
## 📊 Output
Trained Word2Vec model (.model file)

2D scatter plot of word embeddings

Visual clusters of semantically similar words

## 📂 Project Structure
```
├── data/                      # Input text files
├── models/                    # Saved Word2Vec models
├── outputs/                   # Embedding plots and results
├── word2vec_training.py       # Training script
├── visualize_embeddings.py    # Visualization script
├── requirements.txt
└── README.md
```
## 📎 License
This project is licensed under the MIT License. See the LICENSE file for details.

## 🤝 Contributing
Contributions, issues, and feature requests are welcome!
Feel free to open a pull request or an issue.

